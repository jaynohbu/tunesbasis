<div class="grid">
    <div class="col-12 md:col-12">
        <div class="card">
            <h5>Reinforcement Learning</h5>

            <p-tabView orientation="left">

                <!-- INTRODUCTION TAB -->
                <p-tabPanel header="Introduction" class="line-height-3 m-0">
                    <section class="ml-intro">
                        <p>
                            In reinforcement learning (RL), an <strong>agent</strong> interacts with an
                            <strong>environment</strong> over time.
                            At each step, the agent receives the current <strong>state</strong> (a description of the
                            environment),
                            chooses an <strong>action</strong> according to its <strong>policy</strong>, and the
                            environment returns a
                            <strong>reward</strong> and the <strong>next state</strong>.
                        </p>

                        <p>
                            Unlike <strong>supervised learning</strong>, where a model is trained once on a fixed
                            dataset of
                            <em>labeled examples</em>,
                            <mark>Reinforcement Learning (RL) learns while interacting</mark> with its environment.
                            After each action it
                            <strong>updates internal parameters</strong>
                            (stored inside a <span style="color:#2a6ebb; font-weight:bold;">table</span>,
                            <span style="color:#2a6ebb; font-weight:bold;">function approximator</span>,
                            or <span style="color:#2a6ebb; font-weight:bold;">neural network</span>)
                            that represent its <strong>policy</strong> or its <strong>value function</strong>.
                        </p>

                        <p>
                            This <mark>stored representation is the agent‚Äôs ‚Äúmemory‚Äù of how to act</mark>;
                            it accumulates <strong>experience</strong> instead of being given
                            <em>correct answers</em>.
                        </p>
                        <p>
                            Over time the agent improves its policy so that it chooses actions that maximize the total
                            (cumulative) reward.
                            Some algorithms do this by estimating a <strong>value function</strong>‚Äîhow good it is to be
                            in a state or to take a
                            specific action‚Äîwhich then guides the policy,
                            while others directly optimize the policy itself.
                            This continual, trial-and-error updating is what distinguishes RL from other
                            machine-learning paradigms.
                        </p>


                        <!-- AGENT‚ÄìENVIRONMENT GRAPHIC -->
                        <div style="text-align:center; margin: 2em 0;">
                            <svg width="500" height="200">
                                <!-- Agent box -->
                                <rect x="50" y="60" width="120" height="80" fill="#4F9EFF" stroke="#333"
                                    stroke-width="2" rx="10" />
                                <text x="110" y="105" font-size="14" text-anchor="middle" fill="#fff">Agent</text>

                                <!-- Environment box -->
                                <rect x="330" y="60" width="120" height="80" fill="#A3D977" stroke="#333"
                                    stroke-width="2" rx="10" />
                                <text x="390" y="105" font-size="14" text-anchor="middle" fill="#fff">Environment</text>

                                <!-- Action arrow -->
                                <line x1="170" y1="80" x2="330" y2="80" stroke="#333" stroke-width="2"
                                    marker-end="url(#arrowhead)" />
                                <text x="250" y="70" font-size="12" text-anchor="middle">Action</text>

                                <!-- State/Reward arrow -->
                                <line x1="330" y1="120" x2="170" y2="120" stroke="#333" stroke-width="2"
                                    marker-end="url(#arrowhead2)" />
                                <text x="250" y="140" font-size="12" text-anchor="middle">State + Reward</text>

                                <defs>
                                    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5"
                                        orient="auto">
                                        <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
                                    </marker>
                                    <marker id="arrowhead2" markerWidth="10" markerHeight="7" refX="0" refY="3.5"
                                        orient="auto">
                                        <polygon points="10 0, 0 3.5, 10 7" fill="#333" />
                                    </marker>
                                </defs>
                            </svg>
                        </div>
                     <div class="side-note">
  <h3>Types of Reinforcement Learning</h3>

  <ul>
    <li>
      <strong>Model-based:</strong>
      <mark>
        Methods that <em>build an internal model of state transitions and/or rewards</em>
        to <em>predict or simulate what will happen next</em> and plan ahead.
      </mark>
    </li>

    <li>
      <strong>Model-free:</strong>
      The Cross-Entropy Method does <em>not</em> build such a model;
      it learns directly from experience without
      <mark>predicting or simulating the environment</mark>.
    </li>

    <li>
      <strong>Policy-based:</strong>
      Learns the <em>policy itself</em> ‚Äî a probability distribution over actions for each state ‚Äî
      instead of first estimating Q-values and then deriving a policy.
    </li>

    <li>
      <strong>On-policy:</strong>
      Uses <em>fresh data generated by the current policy</em> for its updates,
      unlike off-policy methods that can learn from old or other policies‚Äô data.
    </li>
  </ul>

  <p>
    <em>Policy-based</em> describes <strong>what</strong> the method learns (a direct policy vs. a value function),
    whereas <em>on-policy</em> describes <strong>how</strong> the data is collected
    (fresh data from the current policy vs. replay/other policies).
  </p>
</div>


                    </section>


                </p-tabPanel>

                <!-- CROSS-ENTROPY METHOD TAB -->
                <p-tabPanel header="Cross-Entropy Method">
                    <div class="sub-section">
                        <h3>Cross-Entropy Method (CEM)</h3>
                        <p>
                            The Cross-Entropy Method is a stochastic optimization technique. It evaluates a population
                            of solutions,
                            keeps the best (‚Äúelite‚Äù) samples, and updates the distribution toward them by minimizing
                            cross-entropy. In
                            this context the model‚Äôs policy is a parameterized distribution <code>q(x; Œ∏)</code>, and
                            the elite samples
                            define the target distribution <code>p(x)</code>. Each iteration adjusts <code>Œ∏</code> to
                            minimize the
                            cross-entropy:
                        </p>
                        <p style="text-align:center;">
                            <code>min<sub>Œ∏</sub> H(p, q<sub>Œ∏</sub>) = ‚Äì Œ£ p(x) log q(x; Œ∏)</code>
                        </p>
                        <p>
                            This minimization ‚Äúcross-entrophies away‚Äù bad behaviors by steadily concentrating the
                            probability mass of
                            <code>q(x; Œ∏)</code> on the elite (high-reward) regions of <code>p(x)</code>, thereby
                            optimizing the policy
                            step by step.
                        </p>

                        <h4>Formula (Cross-Entropy): Additional Information Burden</h4>
                        <p>
                            H(p, q) = ‚Äì Œ£ p(x) log q(x)<br>
                            ‚Ä¢ Measures how well q approximates true distribution p.<br>
                            ‚Ä¢ Equivalent to "extra bits" needed if samples come from p but described by q".<br>
                            ‚Ä¢ This Additional Information Burden represents the Cross-Entropy.
                        </p>

                        <img src="assets/images/cross_entropy_example.png" alt="Cross-Entropy p vs q Example"
                            style="max-width:50%; margin:15px 0;">

                        <!-- Various side notes inside Cross-Entropy Tab -->
                        <div class="side-note">
                            <h4>Why ‚ÄúCross‚Äù?</h4>
                            <p>
                                It‚Äôs called <strong>cross-entropy</strong> because it ‚Äúcrosses‚Äù two distributions: the
                                <strong>true distribution (p)</strong> and the <strong>model distribution (q)</strong>.
                            </p>
                        </div>

                        <div class="side-note">
                            <h4>Information Theory View</h4>
                            <p>
                                ‚Ä¢ Entropy H(p): average number of bits to encode events from p. <br>
                                ‚Ä¢ Cross-Entropy H(p,q): bits needed if we use q to encode p. <br>
                                ‚Ä¢ KL Divergence: H(p,q) ‚Äì H(p) = wasted extra bits.
                            </p>
                            <p>
                                üëâ Cross-Entropy answers: ‚ÄúHow inefficient is my model q when the world really follows
                                p?‚Äù
                            </p>
                        </div>

                        <div class="side-note">
                            <h4>Why the log?</h4>
                            <p>
                                Information content = ‚Äìlog‚ÇÇ(p).
                                ‚Ä¢ Common events (p high) ‚Üí small information.
                                ‚Ä¢ Rare events (p low) ‚Üí large information.
                            </p>
                            <img src="assets/images/information_content.png" alt="Information Content Curve"
                                style="max-width:50%; margin:15px 0;">
                            <p>
                                Example points: p=0.5 ‚Üí 1 bit, p=0.25 ‚Üí 2 bits, p=0.1 ‚Üí 3.32 bits. This explains why log
                                is natural: it
                                maps probability to ‚Äúbits of surprise.‚Äù
                            </p>
                        </div>

                        <div class="side-note">
                            <h4>Intuition Summary</h4>
                            <ul>
                                <li>Entropy = baseline uncertainty of true distribution p.</li>
                                <li>Cross-Entropy = coding cost when using q for p.</li>
                                <li>KL Divergence = inefficiency (extra cost) from mismatch.</li>
                            </ul>
                        </div>
                        <!-- SIDE NOTE ABOUT CEM -->
                        <div class="side-note">
                            <h3>Key Characteristics of the Cross-Entropy Method</h3>

                            <p>
                                Put together, the <strong>Cross-Entropy Method</strong> belongs at the
                                <mark>intersection of model-free + policy-based + on-policy</mark>.
                            </p>
                        </div>
                        <div class="side-note">
                            <h3>Where the Cross-Entropy Method Fits</h3>
                            <p>
                                The <strong>Cross-Entropy Method</strong> belongs in the <strong>model-free</strong> +
                                <strong>policy-based</strong> + <strong>on-policy</strong> category of reinforcement
                                learning.
                            </p>
                            <ul>
                                <li><strong>Model-free:</strong> Does not build a model of state transitions or rewards.
                                </li>
                                <li><strong>Policy-based:</strong> Directly learns the policy instead of Q-values.</li>
                                <li><strong>On-policy:</strong> Requires fresh data from the current policy at each
                                    iteration.</li>
                            </ul>
                            <p>
                                In the three-circle diagram of RL approaches, it sits exactly at the intersection of all
                                three circles.
                            </p>
                        </div>
                    </div>

                    <!-- ===================== CartPole Problem Section ===================== -->
                <section class="cartpole-description">
  <h2>CartPole Environment and Cross-Entropy Method</h2>

  <p>
    The <strong>CartPole</strong> environment continuously provides the agent with a state vector describing what is happening ‚Äî
    how far the cart is from the center, how fast it is moving, how tilted the pole is and how quickly it is tipping.
    At first the policy (for example, a small neural network trained with the <strong>Cross-Entropy Method</strong>)
    picks actions almost at random, outputting rough probabilities for pushing the cart left or right.
  </p>

  <p>
    Each <strong>step</strong> is one such interaction ‚Äî seeing a state, taking an action, and receiving the next state and reward ‚Äî
    while an <strong>episode</strong> is the full run of many steps from start until the pole falls or the time limit ends.
    After running many episodes the method scores each run by its total reward and then
    <strong>throws away the low-reward (‚Äúnon-elite‚Äù) episodes</strong>, keeping only the highest-reward (‚Äúelite‚Äù) ones.
    These elite episodes define a new ‚Äútarget‚Äù action distribution <span>p</span> ‚Äî what good behaviour looks like ‚Äî and the
    network parameters of its current policy <span>q<sub>&theta;</sub></span> are updated so that its output probabilities in similar states
    become closer to that elite distribution.
  </p>

  <p>
    This process ‚Äî <em>discarding the ‚Äúextra bits‚Äù of information from poor episodes and aligning with the elite distribution</em> ‚Äî
    is exactly why it‚Äôs called the <strong>Cross-Entropy Method</strong>:
    it minimises the cross-entropy between the model‚Äôs distribution <span>q<sub>&theta;</sub></span> and the empirical distribution
    <span>p</span> from elite episodes, effectively removing (‚Äúcompressing away‚Äù) the inefficient, low-reward behaviours and
    concentrating probability mass on high-reward behaviours.
    With every round the random guesses become more informed, and the policy gradually learns which moves keep the pole balanced,
    without ever building a physics model of the environment but purely by shifting its action probabilities toward those seen
    in successful trajectories.
  </p>
</section>

                    <!-- ===================== CartPole Code Samples (no imports) ===================== -->
                    <section class="cartpole-code">
                        <h2>CartPole Cross-Entropy Python Code</h2>

                        <ng-container ngNonBindable>
                            <h3>1. The Policy Network</h3>
                            <pre><code class="language-python">
class Net(nn.Module):
    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):
        super(Net, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, n_actions)
        )
    def forward(self, x: torch.Tensor):
        return self.net(x)
    </code></pre>
                        </ng-container>

                        <ng-container ngNonBindable>
                            <h3>2. Episode Data Structures</h3>
                            <pre><code class="language-python">
@dataclass
class EpisodeStep:
    observation: np.ndarray
    action: int

@dataclass
class Episode:
    reward: float
    steps: tt.List[EpisodeStep]
    </code></pre>
                        </ng-container>

                        <ng-container ngNonBindable>
                            <h3>3. Collecting Batches of Episodes</h3>
                            <pre><code class="language-python">
def iterate_batches(env: gym.Env, net: Net, batch_size: int) -> tt.Generator[tt.List[Episode], None, None]:
    batch: tt.List[Episode] = []
    episode_reward = 0.0
    episode_steps: tt.List[EpisodeStep] = []
    obs, _ = env.reset()
    sm = nn.Softmax(dim=1)

    while True:
        obs_v = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
        logits_v = net(obs_v)
        act_probs_v = sm(logits_v)
        act_probs = act_probs_v.detach().cpu().numpy()[0]
        action = np.random.choice(len(act_probs), p=act_probs)

        next_obs, reward, terminated, truncated, _ = env.step(action)
        is_done = terminated or truncated
        episode_reward += float(reward)
        episode_steps.append(EpisodeStep(observation=obs, action=action))

        if is_done:
            batch.append(Episode(reward=episode_reward, steps=episode_steps))
            episode_reward = 0.0
            episode_steps = []
            next_obs, _ = env.reset()
            if len(batch) == batch_size:
                yield batch
                batch = []
        obs = next_obs
    </code></pre>
                        </ng-container>

                        <ng-container ngNonBindable>
                            <h3>4. Filtering for Elite Episodes</h3>
                            <pre><code class="language-python">
def filter_batch(batch: tt.List[Episode], percentile: float):
    rewards = [e.reward for e in batch]
    reward_bound = float(np.percentile(rewards, percentile))
    reward_mean = float(np.mean(rewards))
    train_obs, train_act = [], []
    for episode in batch:
        if episode.reward < reward_bound:
            continue
        train_obs.extend([step.observation for step in episode.steps])
        train_act.extend([step.action for step in episode.steps])
    train_obs_v = torch.FloatTensor(np.vstack(train_obs))
    train_act_v = torch.LongTensor(train_act)
    return train_obs_v, train_act_v, reward_bound, reward_mean
    </code></pre>
                        </ng-container>

                        <ng-container ngNonBindable>
                            <h3>5. Main Training Loop</h3>
                            <pre><code class="language-python">
if __name__ == "__main__":
    env = gym.make("CartPole-v1")
    obs_size = env.observation_space.shape[0]
    n_actions = int(env.action_space.n)
    net = Net(obs_size, HIDDEN_SIZE, n_actions)
    print(net)
    objective = nn.CrossEntropyLoss()
    optimizer = optim.Adam(params=net.parameters(), lr=0.01)
    writer = SummaryWriter(comment="-cartpole")

    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):
        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)
        optimizer.zero_grad()
        action_scores_v = net(obs_v)
        loss_v = objective(action_scores_v, acts_v)
        loss_v.backward()
        optimizer.step()
       
        if reward_m > 475:
            print("Solved!")
            break
    writer.close()
    </code></pre>
                        </ng-container>

                    </section>
                </p-tabPanel>

                <!-- VALUE-BASED + OTHER METHODS TAB -->
                <p-tabPanel header="Value-based Methods + Other Methods" class="line-height-3 m-0">
                    <h1>Value-based Methods and Other Approaches</h1>

                    <p-tabView orientation="top">

                     <p-tabPanel header="Bellman Equation">
  <div class="sub-section">
    <h3>Tabular Learning and the Bellman Equation</h3>
    <p>
      We want a simple rule for ‚Äúhow good things are‚Äù so an agent can choose better actions.
      The key idea is:
      <strong>Value now = reward now + discounted value later</strong>.
    </p>

    <!-- Intuition -->
    <h4>üîπ Intuition</h4>
    <p>
      From a state <code>s</code>, if you take an action <code>a</code> you get an immediate reward
      <code>r(s,a)</code> and land in a next state <code>s'</code> whose ‚Äúgoodness‚Äù is captured by its value.
      So a specific action‚Äôs quality is:
      <code>Q(s,a) = r(s,a) + Œ≥ ¬∑ V(s')</code>.
    </p>

    <!-- Small inline diagram -->
    <div style="margin:1rem 0; border:1px solid #e5e7eb; border-radius:12px; padding:12px;">
      <svg viewBox="0 0 660 200" width="100%" height="auto" role="img" aria-label="Bellman diagram">
        <!-- Nodes -->
        <rect x="30"  y="70"  rx="10" ry="10" width="130" height="60" fill="#f0f9ff" stroke="#60a5fa"/>
        <text x="95" y="105" text-anchor="middle" font-size="16">State s</text>

        <rect x="260" y="20"  rx="10" ry="10" width="150" height="50" fill="#eef2ff" stroke="#818cf8"/>
        <text x="335" y="50" text-anchor="middle" font-size="14">Action a</text>

        <rect x="480" y="70"  rx="10" ry="10" width="150" height="60" fill="#ecfccb" stroke="#84cc16"/>
        <text x="555" y="95" text-anchor="middle" font-size="16">Next State s'</text>
        <text x="555" y="115" text-anchor="middle" font-size="13">value V(s')</text>

        <!-- Arrows -->
        <defs>
          <marker id="arrow" markerWidth="10" markerHeight="10" refX="6" refY="3" orient="auto">
            <path d="M0,0 L0,6 L9,3 z" fill="#64748b" />
          </marker>
        </defs>

        <line x1="160" y1="100" x2="260" y2="45" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)"/>
        <text x="210" y="70" text-anchor="middle" font-size="13">choose</text>

        <line x1="410" y1="45" x2="480" y2="100" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)"/>
        <text x="445" y="70" text-anchor="middle" font-size="13">leads to</text>

        <!-- Rewards and formula -->
        <text x="335" y="95" text-anchor="middle" font-size="13">reward r(s,a)</text>
        <text x="335" y="150" text-anchor="middle" font-size="15">
          Q(s,a) = r(s,a) + Œ≥ ¬∑ V(s')
        </text>
      </svg>
    </div>

    <!-- Bellman equations -->
    <h4>üîπ Bellman (Easy Form)</h4>
    <ul>
      <li><strong>Action Value (optimal):</strong> <code>Q*(s,a) = r(s,a) + Œ≥ ¬∑ max<sub>a'</sub> Q*(s',a')</code></li>
      <li><strong>State Value (optimal):</strong> <code>V*(s) = max<sub>a</sub> Q*(s,a)</code></li>
    </ul>
    <p style="margin-top:-6px;">
      (Under a fixed policy œÄ, replace the <code>max</code> with an average:
      <code>V<sup>œÄ</sup>(s) = Œ£<sub>a</sub> œÄ(a|s) ¬∑ Q<sup>œÄ</sup>(s,a)</code>.)
    </p>

    <!-- Clear distinction note -->
    <h4>üîπ State Value vs Action Value (the key distinction)</h4>
    <div class="side-note" style="border-left:4px solid #22c55e; padding-left:12px;">
      <p style="margin:0.2rem 0;">
        <strong>V(s):</strong> ‚ÄúHow good is it to be in state <em>s</em> if I follow my plan (policy) from now on?‚Äù
      </p>
      <p style="margin:0.2rem 0;">
        <strong>Q(s,a):</strong> ‚ÄúHow good is it to take this <em>specific action a</em> in state <em>s</em> right now,
        then follow my plan?‚Äù
      </p>
      <p style="margin:0.2rem 0;">
        Under a fixed policy œÄ:
        <code>V<sup>œÄ</sup>(s) = Œ£<sub>a</sub> œÄ(a|s) ¬∑ Q<sup>œÄ</sup>(s,a)</code>
        (a policy-weighted average over actions).
      </p>
      <p style="margin:0.2rem 0;">
        Under the optimal policy:
        <code>V*(s) = max<sub>a</sub> Q*(s,a)</code>
        (pick the best first action).
      </p>
    </div>

    <!-- Why Q(s,a) = r + Œ≥ V(s') makes sense -->
    <h4>üîπ Why does <code>Q(s,a) = r(s,a) + Œ≥ ¬∑ V(s')</code> make sense?</h4>
    <p>
      Taking <code>a</code> in <code>s</code> gives an immediate payoff <code>r(s,a)</code>.
      After that you‚Äôre in <code>s'</code>, and the future from there is exactly ‚Äúhow good <code>s'</code> is,‚Äù i.e.,
      <code>V(s')</code>. Discount by <code>Œ≥</code> and add them up. That‚Äôs the total goodness of choosing <code>a</code> now.
    </p>

    <!-- Tiny numeric example -->
    <h4>üîπ Tiny Example</h4>
    <p>
      Suppose from state <code>s</code>:
      <br>‚Ä¢ Action A: <code>r=2</code>, next state has <code>V(s')=10</code>
      <br>‚Ä¢ Action B: <code>r=5</code>, next state has <code>V(s')=6</code>
      <br>With <code>Œ≥=0.9</code>:
      <br><code>Q(s,A)=2+0.9¬∑10=11</code>, <code>Q(s,B)=5+0.9¬∑6=10.4</code>
      <br>So <code>V(s)=max(Q)=11</code> by choosing A.
    </p>

    <!-- FrozenLake note -->
    <h4>üîπ In Practice (FrozenLake)</h4>
    <p>
      Each tile is a state. <strong>V(s)</strong> tells how promising the tile is overall.
      <strong>Q(s,a)</strong> tells how promising it is to move (Left/Down/Right/Up) from that tile.
      Tabular methods (value iteration, Q-learning) apply these equations to fill and refine the tables until they stabilize.
    </p>
  </div>
</p-tabPanel>


                        <p-tabPanel header="Deep Q-Networks">
                            <div class="sub-section">
                                <h3>Deep Q-Networks (DQN)</h3>
                                <p>From tabular Q-learning to deep Q-learning, training loop, Pong/Atari examples,
                                    wrappers,
                                    performance issues.</p>
                            </div>
                        </p-tabPanel>

                        <p-tabPanel header="Higher-level RL Libraries">
                            <div class="sub-section">
                                <h3>Higher-level RL Libraries</h3>
                                <p>Using abstractions (PTAN, replay buffers, experience sources, target networks).</p>
                            </div>
                        </p-tabPanel>

                        <p-tabPanel header="DQN Extensions">
                            <div class="sub-section">
                                <h3>DQN Extensions</h3>
                                <p>Variants of DQN (Double DQN, Dueling DQN, Prioritized Replay, etc.).</p>
                            </div>
                        </p-tabPanel>

                        <p-tabPanel header="Speeding up RL">
                            <div class="sub-section">
                                <h3>Ways to Speed Up RL</h3>
                                <p>Engineering & algorithmic optimizations: parallelization, reward shaping, curriculum
                                    learning.</p>
                            </div>
                        </p-tabPanel>

                        <p-tabPanel header="RL for Stock Trading">
                            <div class="sub-section">
                                <h3>Stocks Trading Using RL</h3>
                                <p>Designing a stock trading environment, training an RL agent, evaluating trading
                                    strategies.</p>
                            </div>
                        </p-tabPanel>

                    </p-tabView>
                </p-tabPanel>

            </p-tabView>
        </div>
    </div>
</div>