<div class="grid">
    <div class="col-12 md:col-12">
        <div class="card">
            <h5>Reinforcement Learning</h5>
            <p-tabView orientation="left">

                <!-- INTRODUCTION -->
                <p-tabPanel header="Introduction" class="line-height-3 m-0">
                    <h1>Introduction to Reinforcement Learning</h1>

                    <p-tabView orientation="top">

                     
                        <!-- Chapter 4 -->
                        <p-tabPanel header="Cross-Entropy Method">
                            <div class="sub-section">
                                <h3>Cross-Entropy Method (CEM)</h3>
                                <p>
                                    The Cross-Entropy Method is a stochastic optimization technique.
                                    It evaluates a population of solutions, keeps the best (‚Äúelite‚Äù) samples,
                                    and updates the distribution toward them by minimizing cross-entropy.
                                    In this context the model‚Äôs policy is a parameterized distribution
                                    <code>q(x; Œ∏)</code>, and the elite samples define the target distribution
                                    <code>p(x)</code>.
                                    Each iteration adjusts <code>Œ∏</code> to minimize the cross-entropy:
                                </p>
                                <p style="text-align:center;">
                                    <code>min<sub>Œ∏</sub> H(p, q<sub>Œ∏</sub>) = ‚Äì Œ£ p(x) log q(x; Œ∏)</code>
                                </p>
                                <p>
                                    This minimization ‚Äúcross-entrophies away‚Äù bad behaviors by steadily
                                    concentrating the probability mass of <code>q(x; Œ∏)</code> on the elite
                                    (high-reward) regions of <code>p(x)</code>, thereby optimizing the policy step by
                                    step.
                                </p>


                                <h4>Formula (Cross-Entropy): Viewing One's Own Uncertainty in Comparison with Others =
                                    Additional Information Burden</h4>

                                <p>
                                    H(p, q) = ‚Äì Œ£ p(x) log q(x)
                                    <br>
                                    ‚Ä¢ Measures how well q approximates true distribution p.
                                    <br>
                                    ‚Ä¢ Equivalent to "extra bits" needed if samples come from p but described by q".
                                    <br>
                                    ‚Ä¢ This Additional Information Burden represents the Cross-Entropy.
                                </p>

                                <img src="assets/images/cross_entropy_example.png" alt="Cross-Entropy p vs q Example"
                                    style="max-width:50%; margin:15px 0;">

                                <div class="side-note">
                                    <h4>Why ‚ÄúCross‚Äù?</h4>
                                    <p>
                                        It‚Äôs called <strong>cross-entropy</strong> because it ‚Äúcrosses‚Äù two
                                        distributions:
                                        the <strong>true distribution (p)</strong> and the <strong>model distribution
                                            (q)</strong>.
                                        It‚Äôs not about impurity, but about mismatch between two probability views.
                                    </p>
                                </div>

                                <div class="side-note">
                                    <h4>Information Theory View</h4>
                                    <p>
                                        ‚Ä¢ Entropy H(p): average number of bits to encode events from p. <br>
                                        ‚Ä¢ Cross-Entropy H(p,q): bits needed if we use q to encode p. <br>
                                        ‚Ä¢ KL Divergence: H(p,q) ‚Äì H(p) = wasted extra bits.
                                    </p>
                                    <p>
                                        üëâ Cross-Entropy answers: ‚ÄúHow inefficient is my model q when the world really
                                        follows p?‚Äù
                                    </p>
                                </div>

                                <div class="side-note">
                                    <h4>Why the log?</h4>
                                    <p>
                                        Information content = ‚Äìlog‚ÇÇ(p).
                                        ‚Ä¢ Common events (p high) ‚Üí small information.
                                        ‚Ä¢ Rare events (p low) ‚Üí large information.
                                    </p>
                                    <img src="assets/images/information_content.png" alt="Information Content Curve"
                                        style="max-width:50%; margin:15px 0;">
                                    <p>
                                        Example points: p=0.5 ‚Üí 1 bit, p=0.25 ‚Üí 2 bits, p=0.1 ‚Üí 3.32 bits.
                                        This explains why log is natural: it maps probability to ‚Äúbits of surprise.‚Äù
                                    </p>
                                </div>

                                <div class="side-note">
                                    <h4>Intuition Summary</h4>
                                    <ul>
                                        <li>Entropy = baseline uncertainty of true distribution p.</li>
                                        <li>Cross-Entropy = coding cost when using q for p.</li>
                                        <li>KL Divergence = inefficiency (extra cost) from mismatch.</li>
                                    </ul>
                                </div>
                            </div>

                        </p-tabPanel>

                    </p-tabView>
                </p-tabPanel>

                <!-- PART 2 -->
                <p-tabPanel header="Value-based Methods + Other Methods" class="line-height-3 m-0">
                    <h1>Value-based Methods and Other Approaches</h1>

                    <p-tabView orientation="top">

                        <!-- Chapter 5 -->
                        <p-tabPanel header="Bellman Equation">
                            <div class="sub-section">
                                <h3>Tabular Learning and the Bellman Equation</h3>
                                <p>Value functions, Bellman equation, Q-iteration, FrozenLake experiments.</p>
                            </div>
                        </p-tabPanel>

                        <!-- Chapter 6 -->
                        <p-tabPanel header="Deep Q-Networks">
                            <div class="sub-section">
                                <h3>Deep Q-Networks (DQN)</h3>
                                <p>From tabular Q-learning to deep Q-learning, training loop, Pong/Atari examples,
                                    wrappers, performance issues.</p>
                            </div>
                        </p-tabPanel>

                        <!-- Chapter 7 -->
                        <p-tabPanel header="Higher-level RL Libraries">
                            <div class="sub-section">
                                <h3>Higher-level RL Libraries</h3>
                                <p>Using abstractions (PTAN, replay buffers, experience sources, target networks).</p>
                            </div>
                        </p-tabPanel>

                        <!-- Chapter 8 -->
                        <p-tabPanel header="DQN Extensions">
                            <div class="sub-section">
                                <h3>DQN Extensions</h3>
                                <p>Variants of DQN (Double DQN, Dueling DQN, Prioritized Replay, etc.).</p>
                            </div>
                        </p-tabPanel>

                        <!-- Chapter 9 -->
                        <p-tabPanel header="Speeding up RL">
                            <div class="sub-section">
                                <h3>Ways to Speed Up RL</h3>
                                <p>Engineering & algorithmic optimizations: parallelization, reward shaping, curriculum
                                    learning.</p>
                            </div>
                        </p-tabPanel>

                        <!-- Chapter 10 -->
                        <p-tabPanel header="RL for Stock Trading">
                            <div class="sub-section">
                                <h3>Stocks Trading Using RL</h3>
                                <p>Designing a stock trading environment, training an RL agent, evaluating trading
                                    strategies.</p>
                            </div>
                        </p-tabPanel>

                    </p-tabView>
                </p-tabPanel>

            </p-tabView>
        </div>
    </div>
</div>